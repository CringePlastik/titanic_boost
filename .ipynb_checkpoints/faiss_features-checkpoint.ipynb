{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import shap\n",
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "import featuretools as ft\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "from typing import Tuple, Callable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "# Features\n",
    "PASSENGER_ID  = 'PassengerId'\n",
    "SURVIVED = 'Survived'\n",
    "PCLASS = 'Pclass'\n",
    "NAME = 'Name'\n",
    "SEX = 'Sex'\n",
    "AGE = 'Age'\n",
    "SIBSP = 'SibSp'\n",
    "PARCH = 'Parch'\n",
    "TICKET = 'Ticket'\n",
    "FARE = 'Fare'\n",
    "CABIN = 'Cabin'\n",
    "EMBARKED = 'Embarked'\n",
    "INCOME_LEVEL = \"income_level\"\n",
    "CAT_FEATURES = [PCLASS, SEX, TICKET, CABIN, EMBARKED]\n",
    "TRANS_PRIMITIVES = ['multiply_numeric', 'divide_numeric']\n",
    "\n",
    "DEFAULT_OTHER_NAME = 'Other'\n",
    "DEFAULT_NAN_NAME = 'NaN'\n",
    "DEFAULT_OTHER_FOR_NUMERIC = -99999\n",
    "DEFAULT_NAN_FOR_NUMERIC = -88888\n",
    "TRAIN_FILE = \"data/train.csv\"\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "REPORT_PATH = 'reports/titanic_report.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_FILE)\n",
    "y_train = train[SURVIVED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric_knn(df: pd.DataFrame, imputer_params:dict = {\"n_neighbors\": 5, \"metric\": \"nan_euclidean\", \"weights\": \"uniform\"})->pd.DataFrame:\n",
    "    knn_imputer = KNNImputer(**imputer_params)\n",
    "    transformed_df = knn_imputer.fit_transform(df)\n",
    "    return transformed_df\n",
    "\n",
    "# with Label Encoder    \n",
    "def fill_categorical_label(dataset: pd.DataFrame, column_name: str, fillna=True):\n",
    "    if fillna:\n",
    "        dataset[column_name].fillna(value=DEFAULT_NAN_NAME, inplace=True)\n",
    "    label_encoder = LabelEncoder().fit(np.sort(dataset[column_name].unique()))\n",
    "    dataset[column_name] = dataset[column_name].map(\n",
    "        dict(zip(dataset[column_name].unique(), label_encoder.transform(dataset[column_name].unique()))))\n",
    "    return label_encoder\n",
    "\n",
    "#Target Encoder\n",
    "def fill_categorical_target(dataset: pd.DataFrame, column_name: str, target_column: str, fillna=True):\n",
    "    if fillna:\n",
    "        dataset[column_name].fillna(value=DEFAULT_NAN_NAME, inplace=True)\n",
    "    target_encoder = TargetEncoder()\n",
    "    dataset[column_name] = target_encoder.fit_transform(dataset[column_name], dataset[target_column])\n",
    "    return target_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n"
     ]
    }
   ],
   "source": [
    "train.drop(columns=[PASSENGER_ID, NAME], inplace=True)\n",
    "\n",
    "label_encoded_train = train.copy()\n",
    "for cat_feature in CAT_FEATURES:\n",
    "    fill_categorical_label(label_encoded_train, cat_feature)\n",
    "    \n",
    "target_encoded_train = train.copy()\n",
    "for cat_feature in CAT_FEATURES:\n",
    "    fill_categorical_target(target_encoded_train, cat_feature, SURVIVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "label_encoded_train.drop(SURVIVED, inplace=True, axis=1)\n",
    "target_encoded_train.drop(SURVIVED, inplace=True, axis=1)\n",
    "filled_label_train = pd.DataFrame(fill_numeric_knn(label_encoded_train))\n",
    "filled_target_train = pd.DataFrame(fill_numeric_knn(target_encoded_train))\n",
    "filled_label_train.columns = label_encoded_train.columns\n",
    "filled_target_train.columns = target_encoded_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for adding additional features with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_faiss(df: pd.DataFrame, columns: list, k_neighbors: int=3, nprobe: int=2, stats: list=[\"std\", \"var\", \"median\"])->pd.DataFrame:\n",
    "    n_dimensions = df[columns].shape[1]\n",
    "    quantizer = faiss.IndexFlatL2(n_dimensions)\n",
    "    index = faiss.IndexIVFFlat(quantizer, n_dimensions, k_neighbors, faiss.METRIC_L2)\n",
    "    df_arr = df[columns].to_numpy().astype(np.float32)\n",
    "    df_arr = df_arr.copy(order=\"C\") # if you don't do this you get an exception\n",
    "    print(df_arr.shape)\n",
    "    index.train(df_arr)\n",
    "    index.add(df_arr)\n",
    "    # counting stats\n",
    "    distances, indices = index.search(df_arr, k_neighbors)\n",
    "    out_columns = []\n",
    "    # mean\n",
    "    mean_column = []\n",
    "    for i in indices:\n",
    "            vals = df_arr[i]\n",
    "            mean = np.mean(vals, axis=0)\n",
    "            mean_column.append(mean)\n",
    "    resulting_set = np.array(mean_column)\n",
    "    out_columns.extend([\"mean_\" + col_name for col_name in columns])\n",
    "    \n",
    "    # std\n",
    "    if \"std\" in stats:\n",
    "        std_column = []\n",
    "        for i in indices:\n",
    "            vals = df_arr[i]\n",
    "            std = np.std(vals, axis=0) # std of a all neighbor features\n",
    "            std_column.append(std)\n",
    "        std_column = np.array(std_column)\n",
    "        resulting_set = np.hstack((resulting_set, std_column))\n",
    "        out_columns.extend([\"std_\" + col_name for col_name in columns])\n",
    "    out_df = pd.DataFrame(resulting_set)\n",
    "    out_df.columns = out_columns\n",
    "    return out_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "faissed = add_faiss(filled_label_train, columns=[FARE, SEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 4)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faissed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_Fare</th>\n",
       "      <th>mean_Sex</th>\n",
       "      <th>std_Fare</th>\n",
       "      <th>std_Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.205997</td>\n",
       "      <td>0.643098</td>\n",
       "      <td>0.104489</td>\n",
       "      <td>0.028570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>49.719093</td>\n",
       "      <td>0.465078</td>\n",
       "      <td>0.510082</td>\n",
       "      <td>0.112543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.455566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.137501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033376</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>512.329224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.002338</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean_Fare    mean_Sex    std_Fare     std_Sex\n",
       "count  891.000000  891.000000  891.000000  891.000000\n",
       "mean    32.205997    0.643098    0.104489    0.028570\n",
       "std     49.719093    0.465078    0.510082    0.112543\n",
       "min      0.000000    0.000000    0.000000    0.000000\n",
       "25%      7.910400    0.000000    0.000000    0.000000\n",
       "50%     14.455566    1.000000    0.000000    0.000000\n",
       "75%     31.137501    1.000000    0.033376    0.000000\n",
       "max    512.329224    1.000000    7.002338    0.471405"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faissed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'lossguide',\n",
    "    'n_estimators': 10,\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_lambda': 1,\n",
    "    'max_bin': 120,\n",
    "    'subsample': 0.9\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([16.150455  , 15.67064691, 16.31616044, 16.30536842, 16.43218303,\n",
       "        16.65528512, 16.38747907, 16.71162415, 10.48739409, 10.09708929]),\n",
       " 'score_time': array([0.31923294, 0.29114628, 0.28732944, 0.29925466, 0.28313756,\n",
       "        0.3672688 , 0.32321453, 0.2830627 , 0.19316578, 0.00441194]),\n",
       " 'estimator': (XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None),\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=1, eta=0.02, gamma=0,\n",
       "                gpu_id=-1, grow_policy='lossguide', importance_type='gain',\n",
       "                interaction_constraints='', learning_rate=0.0199999996,\n",
       "                max_bin=120, max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=4,\n",
       "                num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                scale_pos_weight=1, subsample=0.9, tree_method='hist',\n",
       "                validate_parameters=1, verbosity=None)),\n",
       " 'test_accuracy': array([0.77777778, 0.79775281, 0.76404494, 0.82022472, 0.7752809 ,\n",
       "        0.7752809 , 0.74157303, 0.73033708, 0.80898876, 0.76404494]),\n",
       " 'test_f1_macro': array([0.76851852, 0.78809524, 0.74202899, 0.81496881, 0.76871102,\n",
       "        0.75919913, 0.72475461, 0.71103896, 0.79400953, 0.74868899]),\n",
       " 'test_roc_auc': array([0.77428571, 0.81176471, 0.7315508 , 0.8631016 , 0.83502674,\n",
       "        0.83930481, 0.82647059, 0.80481283, 0.84518717, 0.81878307])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_t_scores = cross_validate(xgbc, faissed, y_train, cv=10, scoring=[\"accuracy\", \"f1_macro\", \"roc_auc\"], n_jobs=-1, return_estimator=True)\n",
    "xgb_t_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
